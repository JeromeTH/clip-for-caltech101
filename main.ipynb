{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import scipy\n",
    "import torch\n",
    "from torchvision import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Verify the download by listing the directory contents.\n",
    "# data = datasets.Caltech101(root = \"./data\", target_type = \"category\", download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.Caltech101(root = \"./data\", target_type = \"category\", download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Faces',\n",
       " 'Faces_easy',\n",
       " 'Leopards',\n",
       " 'Motorbikes',\n",
       " 'accordion',\n",
       " 'airplanes',\n",
       " 'anchor',\n",
       " 'ant',\n",
       " 'barrel',\n",
       " 'bass',\n",
       " 'beaver',\n",
       " 'binocular',\n",
       " 'bonsai',\n",
       " 'brain',\n",
       " 'brontosaurus',\n",
       " 'buddha',\n",
       " 'butterfly',\n",
       " 'camera',\n",
       " 'cannon',\n",
       " 'car_side',\n",
       " 'ceiling_fan',\n",
       " 'cellphone',\n",
       " 'chair',\n",
       " 'chandelier',\n",
       " 'cougar_body',\n",
       " 'cougar_face',\n",
       " 'crab',\n",
       " 'crayfish',\n",
       " 'crocodile',\n",
       " 'crocodile_head',\n",
       " 'cup',\n",
       " 'dalmatian',\n",
       " 'dollar_bill',\n",
       " 'dolphin',\n",
       " 'dragonfly',\n",
       " 'electric_guitar',\n",
       " 'elephant',\n",
       " 'emu',\n",
       " 'euphonium',\n",
       " 'ewer',\n",
       " 'ferry',\n",
       " 'flamingo',\n",
       " 'flamingo_head',\n",
       " 'garfield',\n",
       " 'gerenuk',\n",
       " 'gramophone',\n",
       " 'grand_piano',\n",
       " 'hawksbill',\n",
       " 'headphone',\n",
       " 'hedgehog',\n",
       " 'helicopter',\n",
       " 'ibis',\n",
       " 'inline_skate',\n",
       " 'joshua_tree',\n",
       " 'kangaroo',\n",
       " 'ketch',\n",
       " 'lamp',\n",
       " 'laptop',\n",
       " 'llama',\n",
       " 'lobster',\n",
       " 'lotus',\n",
       " 'mandolin',\n",
       " 'mayfly',\n",
       " 'menorah',\n",
       " 'metronome',\n",
       " 'minaret',\n",
       " 'nautilus',\n",
       " 'octopus',\n",
       " 'okapi',\n",
       " 'pagoda',\n",
       " 'panda',\n",
       " 'pigeon',\n",
       " 'pizza',\n",
       " 'platypus',\n",
       " 'pyramid',\n",
       " 'revolver',\n",
       " 'rhino',\n",
       " 'rooster',\n",
       " 'saxophone',\n",
       " 'schooner',\n",
       " 'scissors',\n",
       " 'scorpion',\n",
       " 'sea_horse',\n",
       " 'snoopy',\n",
       " 'soccer_ball',\n",
       " 'stapler',\n",
       " 'starfish',\n",
       " 'stegosaurus',\n",
       " 'stop_sign',\n",
       " 'strawberry',\n",
       " 'sunflower',\n",
       " 'tick',\n",
       " 'trilobite',\n",
       " 'umbrella',\n",
       " 'watch',\n",
       " 'water_lilly',\n",
       " 'wheelchair',\n",
       " 'wild_cat',\n",
       " 'windsor_chair',\n",
       " 'wrench',\n",
       " 'yin_yang']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = sorted(data.categories)  # Sort for consistency\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [data[i][1] for i in range(len(data))]\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/8677\n",
      "320/8677\n",
      "640/8677\n",
      "960/8677\n",
      "1280/8677\n",
      "1600/8677\n",
      "1920/8677\n",
      "2240/8677\n",
      "2560/8677\n",
      "2880/8677\n",
      "3200/8677\n",
      "3520/8677\n",
      "3840/8677\n",
      "4160/8677\n",
      "4480/8677\n",
      "4800/8677\n",
      "5120/8677\n",
      "5440/8677\n",
      "5760/8677\n",
      "6080/8677\n",
      "6400/8677\n",
      "6720/8677\n",
      "7040/8677\n",
      "7360/8677\n",
      "7680/8677\n",
      "8000/8677\n",
      "8320/8677\n",
      "8640/8677\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_images = len(data)\n",
    "image_embeddings = []\n",
    "for i in range(0, num_images, batch_size):\n",
    "    if i % (320) == 0:\n",
    "        print(f\"{i}/{num_images}\")\n",
    "    preprocessed_images = [preprocess(data[i][0]) for i in range(i, min(i+batch_size, num_images))]\n",
    "    image_batch = torch.stack(preprocessed_images).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_emb_batch = clip_model.encode_image(image_batch).cpu()\n",
    "    image_embeddings.append(image_emb_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8677, 512])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings = torch.cat(image_embeddings)\n",
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, image_embeddings, labels):\n",
    "        self.image_embeddings = image_embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_embeddings[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = ClipEmbeddingsDataset(image_embeddings, labels)\n",
    "# embeddings_dataset.__getitem__(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test\n",
    "train_size = int(0.8 * len(embeddings_dataset))\n",
    "test_size = len(embeddings_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(embeddings_dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(512, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 101)\n",
    "        self._init_weights()\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # Applies Xavier initialization\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)  # Sets biases to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch [1/10], Loss: 3.1393\n",
      "Epoch [2/10], Loss: 1.4804\n",
      "Epoch [3/10], Loss: 0.7074\n",
      "Epoch [4/10], Loss: 0.4396\n",
      "Epoch [5/10], Loss: 0.3227\n",
      "Epoch [6/10], Loss: 0.2562\n",
      "Epoch [7/10], Loss: 0.2116\n",
      "Epoch [8/10], Loss: 0.1818\n",
      "Epoch [9/10], Loss: 0.1581\n",
      "Epoch [10/10], Loss: 0.1419\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "model = NeuralNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_epochs= 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()  # Add the loss value for monitoring\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 512]), tensor([94,  0, 45, 50,  1, 92, 71,  5,  3, 53]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label = next(iter(train_dataloader))\n",
    "input.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.3651,  -2.2537,  -3.9694,  ...,  -4.5904,   6.2325,   3.2399],\n",
       "        [ 19.9324,   8.4823,  -1.3478,  ...,  -2.6515,   4.0993,  -1.7972],\n",
       "        [ -3.3025,  -3.6270,  -6.5164,  ...,   2.1753,  -3.0322,   1.5346],\n",
       "        ...,\n",
       "        [  1.4780,  -6.7455,   1.7283,  ..., -11.5951,   2.0453,  -4.8570],\n",
       "        [ -2.1178,  -5.4750,   1.2461,  ...,  -1.3860,  -3.4950,  -7.7456],\n",
       "        [  3.1898,  -4.4681,   2.1757,  ...,  -1.0387,  -1.9317,  -3.8632]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input)\n",
    "output\n",
    "# torch.argmax(model(input), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1272e+00, -1.9285e+00, -3.7517e+00, -4.6694e+00, -5.5210e+00,\n",
       "         -6.2097e+00,  4.7161e+00,  9.9283e+00, -1.9095e+00,  2.4524e+00,\n",
       "          2.5349e+00, -7.4745e-01, -9.7745e-01,  2.8726e+00,  3.2155e-01,\n",
       "         -1.5648e+00,  2.1812e+00, -3.5097e+00, -3.8766e+00, -2.8040e+00,\n",
       "          1.5049e+00, -6.6107e-01, -3.2324e+00, -4.8463e+00, -1.0110e+00,\n",
       "         -4.9893e-01,  4.0377e+00,  4.8096e+00,  4.6827e-01,  2.1204e+00,\n",
       "         -2.9925e+00,  2.9240e+00,  1.1393e+00,  1.3170e-01,  3.3412e+00,\n",
       "         -1.9973e+00,  1.4258e+00,  2.3205e-01, -2.3907e+00, -3.7426e+00,\n",
       "         -2.8767e+00, -2.0958e+00,  4.9416e+00, -1.5762e+00,  4.2071e+00,\n",
       "         -6.3230e+00, -6.9147e+00,  3.0733e-02, -3.4945e+00,  7.9555e-01,\n",
       "         -7.4709e-01,  2.4367e+00, -3.5454e-01,  1.0377e+00,  3.2801e+00,\n",
       "         -4.8946e+00, -2.6122e+00, -2.2284e-01, -9.0631e-01,  4.4204e+00,\n",
       "          1.3482e+00, -1.7854e+00,  6.8808e+00, -3.6229e+00, -5.9012e+00,\n",
       "         -1.6516e+00,  1.0400e+00,  2.8590e+00,  3.8870e+00, -5.3870e-01,\n",
       "         -7.1088e-01,  2.1648e+00, -9.6992e-01,  4.2074e+00,  9.4998e-01,\n",
       "         -2.5981e+00, -1.6313e+00, -6.5007e-03, -2.4778e+00, -2.8964e+00,\n",
       "         -8.3889e-01,  7.5037e+00,  2.2198e+00,  4.0029e-01, -3.7989e+00,\n",
       "         -2.4376e+00,  9.7134e-01,  1.0547e+00,  1.3504e+00,  2.3469e+00,\n",
       "          4.6505e-01,  5.3377e+00,  4.4730e-01, -3.8266e+00, -3.7809e+00,\n",
       "          8.6220e-01, -1.1491e+00,  2.3734e+00, -4.3384e+00, -1.3009e+00,\n",
       "         -1.7331e+00]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(\"sample.jpg\")\n",
    "image2 = data[0][0]\n",
    "preprocessed = preprocess(image).unsqueeze(0)\n",
    "image_embedding = clip_model.encode_image(preprocessed)\n",
    "model(image_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run inference on test_dataloader\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.argmax(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9585253456221198\n",
      "1736\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
